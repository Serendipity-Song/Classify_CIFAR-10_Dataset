{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwjo9kUiBA47UqCfuHGT89",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Serendipity-Song/Classify_CIFAR-10_Dataset/blob/main/Cifar_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZfvM61RgAER",
        "outputId": "36e790c7-78fa-468c-d216-8f28330c2539"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:04<00:00, 42563817.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# 데이터를 로드하고 전처리\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
        "                                         shuffle=False, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import resnet18\n",
        "\n",
        "model = resnet18(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "# 양자화 설정\n",
        "qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
        "model.qconfig = qconfig\n",
        "\n",
        "# 양자화 준비\n",
        "model = torch.quantization.prepare(model, inplace=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wD8eY_bOg2tp",
        "outputId": "7204d940-69ac-4454-b998-8f5908846331"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 112MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (inputs, labels) in enumerate(trainloader, 0):\n",
        "    model(inputs)\n"
      ],
      "metadata": {
        "id": "7cNU9nBNg6t5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.quantization.convert(model, inplace=True)\n"
      ],
      "metadata": {
        "id": "1_swNOqcg9vq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0e3lY11kIfW",
        "outputId": "22d379c0-b688-4ac0-837f-b794e0738eae"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 4s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Normalization\n",
        "X = x_train.astype(np.float32) / 255.0\n",
        "\n",
        "# Define and compile original model\n",
        "base_model = tf.keras.applications.InceptionV3(include_top=False, input_shape=(96, 96, 3), weights=None)\n",
        "x = base_model.output\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = tf.keras.layers.Dense(10, activation='softmax')(x)\n",
        "model = tf.keras.Model(inputs=base_model.input, outputs=x)\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model on CIFAR-10 dataset\n",
        "X_resized = tf.image.resize(X, (96, 96))\n",
        "model.fit(X_resized, y_train, epochs=5)\n",
        "\n",
        "# Generate inputs for DDV computation\n",
        "def generate_inputs(model, X, N=100, epsilon=0.01, lambda_=1.0):\n",
        "    X_prime = X.copy()\n",
        "    for i in range(N):\n",
        "        indices = np.random.choice(X.shape[0], size=int(X.shape[0]*0.1), replace=False)\n",
        "        pos = np.random.choice(np.prod(X.shape[1:]), size=1)\n",
        "        X_prime_left = X_prime.copy()\n",
        "        X_prime_right = X_prime.copy()\n",
        "        X_prime_left[indices, pos//X.shape[2], pos%X.shape[2]] -= epsilon\n",
        "        X_prime_right[indices, pos//X.shape[2], pos%X.shape[2]] += epsilon\n",
        "        score_left = np.abs(model(X_resized) - model(tf.image.resize(X_prime_left, (96, 96)))).sum() + lambda_ * np.abs(X_prime_left - X).sum()\n",
        "        score_right = np.abs(model(X_resized) - model(tf.image.resize(X_prime_right, (96, 96)))).sum() + lambda_ * np.abs(X_prime_right - X).sum()\n",
        "        if score_left > score_right:\n",
        "            X_prime = X_prime_left\n",
        "        else:\n",
        "            X_prime = X_prime_right\n",
        "    return X_prime\n",
        "\n",
        "# Compute DDV\n",
        "def compute_ddv(model, X, X_prime):\n",
        "    return np.abs(model(X).numpy() - model(X_prime).numpy()).sum(axis=1)\n",
        "\n",
        "# Compute model similarity\n",
        "def compute_similarity(DDV_f, DDV_g):\n",
        "    return cosine_similarity([DDV_f], [DDV_g])[0][0]\n",
        "\n",
        "# Generate inputs\n",
        "X_prime = generate_inputs(model, X)\n",
        "\n",
        "# Compute DDVs\n",
        "DDV_f = compute_ddv(model, X_resized, tf.image.resize(X_prime, (96, 96)))\n",
        "DDV_g = compute_ddv(model, X_resized, tf.image.resize(X_prime, (96, 96)))\n",
        "\n",
        "# Compute model similarity\n",
        "similarity = compute_similarity(DDV_f, DDV_g)\n",
        "print(\"Model similarity:\", similarity)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPM7H5Pxj0rG",
        "outputId": "75499378-1f33-419a-9ed6-75c9eacf0004"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            " 563/1563 [=========>....................] - ETA: 51:19 - loss: 1.9552 - accuracy: 0.3166"
          ]
        }
      ]
    }
  ]
}